{
 "cells": [
  {
   "source": [
    "Support Vector Machine and Sequential Minimal Optimization Mathematics and python implementaiton\n",
    "====\n",
    "*Noted by Ziyue Hou on 2020/09/15*\n",
    "\n",
    "## Chapter 0: Introduction\n",
    "\n",
    "General Note to explain how svm works and details how to crete model and calculate coefficients. It mainly includes:\n",
    "* General Workflow of SVM\n",
    "* Mathematics of SMO and it's implementation\n",
    "* SVM implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Chapter 1: General Workflow of SVM\n",
    "\n",
    "### Definition[^1]\n",
    "* Support Vectors:Support vectors are the data points that lie closest\n",
    "to the decision surface (or hyperplane)\n",
    "* Input:  set of (input, output) training pair samples; call the\n",
    "input sample features x1, x2â€¦xn, and the output result y.\n",
    "Typically, there can be lots of input features xi.\n",
    "* Output: \n",
    "set of weights w, one for each feature,\n",
    "whose linear combination predicts the value of y. \n",
    "\n",
    "![img](https://raw.githubusercontent.com/hadleyhzy34/machine_learning/master/SVM/support_vector_machine_from_scratch_files/svm_def.png)\n",
    "\n",
    "[^1]:https://web.mit.edu/6.034/wwwbob/svm.pdf\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Chapter 2: The mathematics of the margin\n",
    "\n",
    "### Hypothesis\n",
    "We want a classifier with as big a margin as possible.\n",
    "Recall the distance from a point\n",
    "<img src=\"https://latex.codecogs.com/svg.latex? (x_{0},y_{0} )\" title=\"x_{ij}\" /> \n",
    "to a line Ax+By+c=0 is:  \n",
    "<img src=\"https://latex.codecogs.com/svg.latex? |A x_{0}+B y_{0}+ c |/sqrt( A^{2} + B^{2}) \" title=\"x_{ij}\" />  \n",
    "Since  \n",
    "<img src=\"https://latex.codecogs.com/svg.latex? | | \\overrightarrow{w}  \\overrightarrow{x} +b  | \" title=\"x_{ij}\" /> for all points that lie on hyperplane is 1, the distance between two hyperplane H0 and H1 is then:\n",
    "<img src=\"https://latex.codecogs.com/svg.latex? | \\overrightarrow{w}  \\overrightarrow{x} +b  |/|| \\overrightarrow{w} || = 1/|| \\overrightarrow{w} || \" title=\"x_{ij}\" /> \n",
    "The hypothesis of SVM is as follows.\n",
    "\n",
    "<img src=\"http://www.sciweavers.org/tex2img.php?eq=%7Bh_%7Bw%2Cb%7D%7D%28x%29%20%3D%20g%28w%5E%7BT%7Dx%20%2B%20b%29%20%3D%20%5Cbegin%7Bcases%7D1%20%26%20%20w%5E%7BT%7Dx%20%2B%20b%20%5Cge%200%5C%5C-1%20%26%20w%5E%7BT%7Dx%20%2B%20b%20%20%3C%20%200%5Cend%7Bcases%7D&bc=White&fc=Black&im=jpg&fs=12&ff=arev&edit=0\" align=\"center\" border=\"0\" alt=\"{h_{w,b}}(x) = g(w^{T}x + b) = \\begin{cases}1 &  w^{T}x + b \\ge 0\\\\-1 & w^{T}x + b  0\\end{cases}\" width=\"351\" height=\"47\" />\n",
    "\n",
    "* *Notice the difference between this and the logististic regression.*\n",
    "* Notice svm is also supervised machine leanring which means y or pypothesis of our model is also input data for our training"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Chapter 3: Minimize <img src=\"https://latex.codecogs.com/svg.latex? || \\overrightarrow{w} || \" title=\"x_{ij}\" />  and solve quadratic programming problem\n",
    "### Our goal is:  \n",
    "max( \n",
    "<img src=\"https://latex.codecogs.com/svg.latex?   \\frac{1}{||\\overrightarrow{w} ||}  \" title=\"x_{ij}\" />\n",
    ")->min(\n",
    "<img src=\"https://latex.codecogs.com/svg.latex? || \\overrightarrow{w}||\" title=\"x_{ij}\" />\n",
    ")->min(\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?  \\frac{1}{2} ||\\overrightarrow{w} || ^{2}\" title=\"x_{ij}\" />\n",
    ")\n",
    "\n",
    "* *Note that square and then multiply by two is better for later derivate computing.\n",
    "\n",
    "### Constrained optimizaiton problem:\n",
    "min(\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?  \\frac{1}{2} ||\\overrightarrow{w} || ^{2}\" title=\"x_{ij}\" />\n",
    ") s.t. <img src=\"https://latex.codecogs.com/svg.latex?  \\\\ {y^{(i)}}({w^T}x + b) \\ge 1,i = 1,...,m\" title=\"x_{ij}\" />\n",
    "\n",
    "### Construct its Lagrangian:  \n",
    "<img src=\"https://latex.codecogs.com/svg.latex?  L(w,b,\\alpha ) = \\frac{1}{2}||w|{|^2} - \\sum\\limits_{i = 1}^m {{\\alpha _i}[{y^{(i)}}({w^T}{x^{(i)}} + b) - 1]}\" title=\"x_{ij}\" />\n",
    "\n",
    "Firstly, we need to get \n",
    "<img src=\"https://latex.codecogs.com/svg.latex?  \\mathop {\\min }\\limits_{w,b} L(w,b,\\alpha )\" title=\"x_{ij}\" />\n",
    "by setting the derivatives of L in respect to w and b as 0:   \n",
    "<img src=\"https://latex.codecogs.com/svg.latex?  {\\nabla _w}L(w,b,\\alpha ) = w - \\sum\\limits_{i = 1}^m {{\\alpha _i}{y^{(i)}}{x^{(i)}}}  = 0\" title=\"x_{ij}\" />  \n",
    "<img src=\"https://latex.codecogs.com/svg.latex?  \\frac{\\partial }{{\\partial b}}L(w,b,\\alpha ) =  - \\sum\\limits_{i = 1}^n {{\\alpha _i}{y^{(i)}}}  = 0\" title=\"x_{ij}\" />  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-c1b068fb227d>, line 4)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-c1b068fb227d>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    define input and initialize alpha\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## SMO Calculation and python Implementation\n",
    "\n",
    "### create model\n",
    "define input and initialize alpha  \n",
    "```python\n",
    "def smo(X, y):\n",
    "    n,d = X.shape[0], X.shape[1]\n",
    "    aplha = np.zeros((n))\n",
    "\n",
    "\n",
    "### from linear model to non-linear model\n",
    "\n",
    "### calculate given current updated alpha\n",
    "```python```\n",
    "self.w = self.calc_w(alpha, y, X)\n",
    "self.b = self.calc_b(X, y, self.w)\n",
    "def calc_b(self, X, y, w):\n",
    "    b_tmp = y - np.dot(w.T, X.T)\n",
    "    return np.mean(b_tmp)\n",
    "def calc_w(self, alpha, y, X):\n",
    "    return np.dot(X.T, np.multiply(alpha,y))\n",
    "def h(self, X, w, b):\n",
    "    return np.sign(np.dot(w.T, X.T) + b).astype(int)\n",
    "```\n",
    "\n",
    "### Prediction error\n",
    "def E(self, x_k, y_k, w, b):\n",
    "    return self.h(x_k, w, b) - y_k\n",
    "\n",
    "### The second derivative of the objective function along the diagonal line can be expressed as:\n",
    "k_ij = kernel(x_i, x_i) + kernel(x_j, x_j) - 2 * kernel(x_i, x_j)\n",
    "\n",
    "### constraint update\n",
    "alpha[j] = alpha_prime_j + float(y_j * (E_i - E_j))/k_ij\n",
    "\n",
    "### clipping constraint\n",
    "alpha[i] = alpha_prime_i + y_i*y_j * (alpha_prime_j - alpha[j])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python_defaultSpec_1600183066694"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}